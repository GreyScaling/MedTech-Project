{"cells":[{"cell_type":"markdown","id":"21019ef1","metadata":{"id":"21019ef1"},"source":["## Required libraries"]},{"cell_type":"code","source":["!pip install praw\n","!pip install PySimpleGUI"],"metadata":{"id":"MzD9KehAaZNP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674467057032,"user_tz":-480,"elapsed":22577,"user":{"displayName":"Ameerul Haq","userId":"17272970661514294606"}},"outputId":"b1a48918-5a3a-4c75-eef3-6fc5446f692d"},"id":"MzD9KehAaZNP","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting praw\n","  Downloading praw-7.6.1-py3-none-any.whl (188 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.8/188.8 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting update-checker>=0.18\n","  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n","Collecting websocket-client>=0.54.0\n","  Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting prawcore<3,>=2.1\n","  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n","Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from prawcore<3,>=2.1->praw) (2.25.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.24.3)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2022.12.7)\n","Installing collected packages: websocket-client, update-checker, prawcore, praw\n","Successfully installed praw-7.6.1 prawcore-2.3.0 update-checker-0.18.0 websocket-client-1.4.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting PySimpleGUI\n","  Downloading PySimpleGUI-4.60.4-py3-none-any.whl (509 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PySimpleGUI\n","Successfully installed PySimpleGUI-4.60.4\n"]}]},{"cell_type":"code","execution_count":null,"id":"ae595f85","metadata":{"id":"ae595f85"},"outputs":[],"source":["import pandas as pd\n","import praw\n","import PySimpleGUI as sg\n","import re"]},{"cell_type":"markdown","id":"92d4a375","metadata":{"id":"92d4a375"},"source":["### Function to locate the text file with all the reddit sites  "]},{"cell_type":"code","execution_count":null,"id":"412142fd","metadata":{"id":"412142fd"},"outputs":[],"source":["def reddit_txt():\n","    while True:\n","        file = sg.popup_get_file(\"Select Plane-Data csv file location\",\n","                                 title='My File Browser',\n","                                 file_types=((\"ALL txt Files\", \"*.txt\"),))\n","        if file != (''):\n","            break\n","        sg.popup_error(' Please enter a file path ')\n","    return file"]},{"cell_type":"markdown","id":"a4d6c5e7","metadata":{"id":"a4d6c5e7"},"source":["### Function to get all the urls from the text file into a list"]},{"cell_type":"code","execution_count":null,"id":"8381662b","metadata":{"id":"8381662b"},"outputs":[],"source":["\n","def getUrls(txtfile):\n","    urls=[]\n","    with open(txtfile) as Fileobj:\n","        for lines in Fileobj:\n","            urls.append(lines)\n","    return urls\n"]},{"cell_type":"markdown","id":"abbb6f33","metadata":{"id":"abbb6f33"},"source":["### Setting reddit api credentials required to be used to use the api"]},{"cell_type":"code","execution_count":null,"id":"7ed17bce","metadata":{"id":"7ed17bce"},"outputs":[],"source":["def getPraw():\n"," return praw.Reddit(user_agent=\"Comment Extraction (by /u/rddit_scrapper)\",\n","                     client_id=\"iD5TP-sX0fWQjXLskZUAsw\", \n","                     client_secret=\"3thlw2TxzNqrs0XsxUzSB7ReX47Cbg\",\n","                     check_for_async=False)"]},{"cell_type":"markdown","id":"0671219e","metadata":{"id":"0671219e"},"source":["### Function uses regex to look for keywords within the url to indicate the type of post and returns its corresponding type"]},{"cell_type":"code","execution_count":null,"id":"5c4161ef","metadata":{"id":"5c4161ef"},"outputs":[],"source":["def trialtype(url):\n","    if re.search('(?i)cancer', url):\n","        return 'cancer'\n","    elif re.search('(?i)(corona)|(covid)|(novavax)', url):\n","        return 'covid'\n","    else:\n","        return 'other'"]},{"cell_type":"markdown","id":"863a3b30","metadata":{"id":"863a3b30"},"source":["### The main function to get all the comments of all the reddit sites "]},{"cell_type":"code","execution_count":null,"id":"4d91ee55","metadata":{"id":"4d91ee55"},"outputs":[],"source":["def getComments(reddit,urls):\n","    \n","    #This list will a nest list of each url's comments \n","    allcommentslist = []\n","    \n","    #loops through the urls list \n","    for url in urls:\n","        \n","        #This list will hold all the comments of the current url only\n","        url_list= []\n","        \n","        #Appending the type of post\n","        url_list.append(trialtype(url))\n","       \n","        #Setting the url \n","        submission = reddit.submission(url=url)\n","        \n","        #All 'More Comments' objects will be replaced until there are none left, \n","        #as long as they satisfy the threshold\n","        submission.comments.replace_more(limit=None)\n","        \n","        #iterating through a list of all the comments and appends it to a list\n","        for comment in submission.comments.list():\n","            url_list.append(comment.body)\n","        \n","        #appending the url list  to the main list to hold all urls' comments\n","        allcommentslist.append(url_list)\n","        \n","    return allcommentslist\n","\n"]},{"cell_type":"code","execution_count":null,"id":"f44f377e","metadata":{"id":"f44f377e"},"outputs":[],"source":["def CreateDataframe(all_comments):\n","    #creates a empty dataframe that will concatinate all urls comments which its type\n","    df = pd.DataFrame([],columns=['comments','type'])\n","    \n","    for comments in all_comments:\n","        #taking out the first value which holds the site's type\n","        comment_type = comments.pop(0)\n","        #Creating a dataframe to hold the comments each loop with the column head as comments\n","        x= pd.DataFrame(comments,columns =['comments'])\n","        #creates a column type to label all the comments with the site's type\n","        x['type'] = comment_type\n","        #concatinate the dataframe with each iteration to hold all the comments labeled with its type\n","        df = pd.concat([df,x])\n","       \n","    return df\n","\n"]},{"cell_type":"markdown","id":"90f90420","metadata":{"id":"90f90420"},"source":["### Function to remove any comments that was deleted or removed on the site"]},{"cell_type":"code","execution_count":null,"id":"50704d39","metadata":{"id":"50704d39"},"outputs":[],"source":["\n","def cleanDataFrame(df):\n","    indexNames = df[(df.comments == '[removed]') | (df.comments == '[deleted]')].index\n","    df.drop(indexNames, inplace=True)\n","    \n","    return df"]},{"cell_type":"markdown","id":"bd82e221","metadata":{"id":"bd82e221"},"source":["### Function to export the datafram to a csv"]},{"cell_type":"code","execution_count":null,"id":"a1cd5325","metadata":{"id":"a1cd5325"},"outputs":[],"source":["def export_to_csv(df):\n","    #call a GUI to select output folder\n","    destfolder = sg.PopupGetFolder('Please select destination folder for extracted features')\n","    destfolder = destfolder.replace('/','\\\\\\\\')\n","    destfolder += '\\\\\\\\'\n","    df.to_csv(destfolder + 'reddit.csv',index=False)"]},{"cell_type":"markdown","id":"1b8f0c5d","metadata":{"id":"1b8f0c5d"},"source":["### Main function "]},{"cell_type":"code","execution_count":null,"id":"f8459e76","metadata":{"id":"f8459e76"},"outputs":[],"source":["def main():\n","    txtfile = reddit_txt()\n","    reddit = getPraw()\n","    urls = getUrls(txtfile)\n","    all_comments = getComments(reddit,urls)    \n","    df = CreateDataframe(all_comments)\n","    df = cleanDataFrame(df)\n","    export_to_csv(df)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"6bf6900d","metadata":{"id":"6bf6900d"},"outputs":[],"source":["# Calling main function \n","if __name__==\"__main__\": \n","    main()\n"]},{"cell_type":"code","execution_count":null,"id":"2165ef49","metadata":{"id":"2165ef49"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}