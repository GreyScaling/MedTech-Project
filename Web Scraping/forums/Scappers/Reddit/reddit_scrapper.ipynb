{"cells":[{"cell_type":"markdown","id":"7a8a8a2d","metadata":{},"source":["# Reddit Comment Scraper Documentation"]},{"cell_type":"markdown","id":"7bda68d9","metadata":{},"source":["## Introduction\n","The Reddit Comment Scraper is a Python script that extracts comments from Reddit posts based on a list of URLs provided in a text file. \n","It uses the PRAW library (Python Reddit API Wrapper) to interact with the Reddit API and extract comments from various Reddit posts. \n","The extracted data is then processed and exported into a CSV file for further analysis.\n","\n","## Required Libraries\n","- pandas: A library for data manipulation and analysis.\n","- praw: The Python Reddit API Wrapper for accessing Reddit's API.\n","- PySimpleGUI: A simple graphical user interface (GUI) library.\n","- re: The regular expressions library for pattern matching.\n","\n","\n","## Functions\n","1. reddit_txt(): This function is responsible for opening a GUI window that allows the user to select a text file containing a list of Reddit URLs. It returns the selected file's path.\n","\n","2. getUrls(txtfile): This function takes the path of a text file as input and reads the URLs from the file, storing them in a list. It then returns this list of URLs.\n","\n","3. getPraw(): This function initializes and configures the Reddit API using the provided credentials. It returns a Reddit API instance.\n","\n","4. trialtype(url): This function uses regular expressions to determine the type of a Reddit post based on the keywords present in the URL. It returns the type of the post (\"cancer\", \"covid\", or \"other\").\n","\n","5. getComments(reddit, urls): The main function responsible for extracting comments from Reddit posts. It takes the Reddit API instance and the list of URLs as inputs. It iterates through each URL, extracts comments, and organizes them into a nested list based on the post type. The nested list is then returned.\n","\n","6. CreateDataframe(all_comments): This function creates a Pandas DataFrame to hold the comments extracted from Reddit. It takes the nested list of comments as input, processes them, and organizes them into the DataFrame with two columns: \"comments\" and \"type\".\n","\n","7. cleanDataFrame(df): This function removes any comments that have been deleted or removed from the DataFrame. It takes the DataFrame as input, drops rows with deleted or removed comments, and returns the cleaned DataFrame.\n","\n","8. export_to_csv(df): This function prompts the user to select an output folder using a GUI window and exports the cleaned DataFrame to a CSV file named \"reddit.csv\" within the selected folder.\n","\n","9. main(): The main function that orchestrates the entire process. It calls the above functions in sequence to scrape Reddit comments, process them, clean the data, and export it to a CSV file.\n","\n","\n","## How to Use\n","- Make sure you have installed the required libraries: pandas, praw, PySimpleGUI.\n","\n","- Replace the client_id and client_secret values in the getPraw() function with your own Reddit API credentials.\n","\n","- Run the script. It will open a GUI window for you to select a text file containing Reddit URLs.\n","\n","- The script will then extract comments from the provided Reddit URLs, process the data, and clean it.\n","\n","- A GUI window will prompt you to select an output folder for the CSV file.\n","\n","- Once the process is complete, the cleaned data will be exported to a CSV file named \"reddit.csv\" within the selected folder."]},{"cell_type":"markdown","id":"21019ef1","metadata":{"id":"21019ef1"},"source":["## Required libraries"]},{"cell_type":"code","execution_count":1,"id":"ae595f85","metadata":{"id":"ae595f85"},"outputs":[],"source":["import pandas as pd\n","import praw\n","import PySimpleGUI as sg\n","import re"]},{"cell_type":"markdown","id":"92d4a375","metadata":{"id":"92d4a375"},"source":["### Function to locate the text file with all the reddit sites  "]},{"cell_type":"code","execution_count":3,"id":"412142fd","metadata":{"id":"412142fd"},"outputs":[],"source":["def reddit_txt():\n","    while True:\n","        file = sg.popup_get_file(\"Select Plane-Data csv file location\",\n","                                 title='My File Browser',\n","                                 file_types=((\"ALL txt Files\", \"*.txt\"),))\n","        if file != (''):\n","            break\n","        sg.popup_error(' Please enter a file path ')\n","    return file"]},{"cell_type":"markdown","id":"a4d6c5e7","metadata":{"id":"a4d6c5e7"},"source":["### Function to get all the urls from the text file into a list"]},{"cell_type":"code","execution_count":4,"id":"8381662b","metadata":{"id":"8381662b"},"outputs":[],"source":["\n","def getUrls(txtfile):\n","    urls=[]\n","    with open(txtfile) as Fileobj:\n","        for lines in Fileobj:\n","            urls.append(lines)\n","    return urls\n"]},{"cell_type":"markdown","id":"abbb6f33","metadata":{"id":"abbb6f33"},"source":["### Setting reddit api credentials required to be used to use the api"]},{"cell_type":"code","execution_count":5,"id":"7ed17bce","metadata":{"id":"7ed17bce"},"outputs":[],"source":["def getPraw():\n"," return praw.Reddit(user_agent=\"Comment Extraction (by /u/rddit_scrapper)\",\n","                     client_id=\"iD5TP-sX0fWQjXLskZUAsw\", \n","                     client_secret=\"3thlw2TxzNqrs0XsxUzSB7ReX47Cbg\",\n","                     check_for_async=False)"]},{"cell_type":"markdown","id":"0671219e","metadata":{"id":"0671219e"},"source":["### Function uses regex to look for keywords within the url to indicate the type of post and returns its corresponding type"]},{"cell_type":"code","execution_count":6,"id":"5c4161ef","metadata":{"id":"5c4161ef"},"outputs":[],"source":["def trialtype(url):\n","    if re.search('(?i)cancer', url):\n","        return 'cancer'\n","    elif re.search('(?i)(corona)|(covid)|(novavax)', url):\n","        return 'covid'\n","    else:\n","        return 'other'"]},{"cell_type":"markdown","id":"863a3b30","metadata":{"id":"863a3b30"},"source":["### The main function to get all the comments of all the reddit sites "]},{"cell_type":"code","execution_count":7,"id":"4d91ee55","metadata":{"id":"4d91ee55"},"outputs":[],"source":["def getComments(reddit,urls):\n","    \n","    #This list will a nest list of each url's comments \n","    allcommentslist = []\n","    \n","    #loops through the urls list \n","    for url in urls:\n","        \n","        #This list will hold all the comments of the current url only\n","        url_list= []\n","        \n","        #Appending the type of post\n","        url_list.append(trialtype(url))\n","       \n","        #Setting the url \n","        submission = reddit.submission(url=url)\n","        \n","        #All 'More Comments' objects will be replaced until there are none left, \n","        #as long as they satisfy the threshold\n","        submission.comments.replace_more(limit=None)\n","        \n","        #iterating through a list of all the comments and appends it to a list\n","        for comment in submission.comments.list():\n","            url_list.append(comment.body)\n","        \n","        #appending the url list  to the main list to hold all urls' comments\n","        allcommentslist.append(url_list)\n","        \n","    return allcommentslist\n","\n"]},{"cell_type":"code","execution_count":8,"id":"f44f377e","metadata":{"id":"f44f377e"},"outputs":[],"source":["def CreateDataframe(all_comments):\n","    #creates a empty dataframe that will concatinate all urls comments which its type\n","    df = pd.DataFrame([],columns=['comments','type'])\n","    \n","    for comments in all_comments:\n","        #taking out the first value which holds the site's type\n","        comment_type = comments.pop(0)\n","        #Creating a dataframe to hold the comments each loop with the column head as comments\n","        x= pd.DataFrame(comments,columns =['comments'])\n","        #creates a column type to label all the comments with the site's type\n","        x['type'] = comment_type\n","        #concatinate the dataframe with each iteration to hold all the comments labeled with its type\n","        df = pd.concat([df,x])\n","       \n","    return df\n","\n"]},{"cell_type":"markdown","id":"90f90420","metadata":{"id":"90f90420"},"source":["### Function to remove any comments that was deleted or removed on the site"]},{"cell_type":"code","execution_count":9,"id":"50704d39","metadata":{"id":"50704d39"},"outputs":[],"source":["\n","def cleanDataFrame(df):\n","    indexNames = df[(df.comments == '[removed]') | (df.comments == '[deleted]')].index\n","    df.drop(indexNames, inplace=True)\n","    \n","    return df"]},{"cell_type":"markdown","id":"bd82e221","metadata":{"id":"bd82e221"},"source":["### Function to export the datafram to a csv"]},{"cell_type":"code","execution_count":10,"id":"a1cd5325","metadata":{"id":"a1cd5325"},"outputs":[],"source":["def export_to_csv(df):\n","    #call a GUI to select output folder\n","    destfolder = sg.PopupGetFolder('Please select destination folder for extracted features')\n","    destfolder = destfolder.replace('/','\\\\\\\\')\n","    destfolder += '\\\\\\\\'\n","    df.to_csv(destfolder + 'reddit.csv',index=False)"]},{"cell_type":"markdown","id":"1b8f0c5d","metadata":{"id":"1b8f0c5d"},"source":["### Main function "]},{"cell_type":"code","execution_count":11,"id":"f8459e76","metadata":{"id":"f8459e76"},"outputs":[],"source":["def main():\n","    txtfile = reddit_txt()\n","    reddit = getPraw()\n","    urls = getUrls(txtfile)\n","    all_comments = getComments(reddit,urls)    \n","    df = CreateDataframe(all_comments)\n","    df = cleanDataFrame(df)\n","    export_to_csv(df)\n","\n"]},{"cell_type":"code","execution_count":12,"id":"6bf6900d","metadata":{"id":"6bf6900d"},"outputs":[],"source":["# Calling main function \n","if __name__==\"__main__\": \n","    main()\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":5}
