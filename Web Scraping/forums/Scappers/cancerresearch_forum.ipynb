{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping for Cancer Research Forum Comments Documentation\n",
    "\n",
    "## Introduction\n",
    "This Python script performs web scraping on the Cancer Research UK forum website to extract comments from a specific thread related to chemotherapy side effects. It uses the BeautifulSoup library to parse the HTML content of the webpage and extract relevant information. The extracted comments are then processed, filtered, and saved into a CSV file for further analysis.\n",
    "\n",
    "## Required Libraries\n",
    "- BeautifulSoup: A library for pulling data out of HTML and XML files.\n",
    "- requests: A library for making HTTP requests.\n",
    "- pandas: A library for data manipulation and analysis.\n",
    "\n",
    "# Script Explanation\n",
    "1. Import Libraries: The script starts by importing the necessary libraries: BeautifulSoup, requests, and pandas.\n",
    "\n",
    "2. URL and Request: The URL of the target forum thread is provided (url_link). The requests.get() function sends an HTTP GET request to the URL, and the HTML content of the response is extracted using the .text attribute.\n",
    "\n",
    "3. HTML Parsing: The HTML content retrieved from the webpage is parsed using BeautifulSoup. The parsed HTML content is stored in the doc variable.\n",
    "\n",
    "4. Extract Comments: The comments are located within the HTML element with the class field-item even. The script uses the find() method to locate this element. Within this element, all <p> (paragraph) tags are found using the find_all('p') method, which returns a list of comment elements.\n",
    "\n",
    "5. Comments Processing: The text content of the comment elements is extracted, converted to strings, and added to the comments list.\n",
    "\n",
    "6. Data Cleaning: The script replaces the '\\xa0' characters (non-breaking spaces) within comments with regular spaces using a list comprehension. Additionally, comments with a length of less than 20 characters are filtered out.\n",
    "\n",
    "7. Create DataFrame: The cleaned comments are organized into a Pandas DataFrame. The DataFrame has two columns: 'comments' and 'type'. The 'type' column is set to 'cancer' for all rows.\n",
    "\n",
    "8. Export to CSV: The DataFrame is exported to a CSV file named 'cancer_research.csv'. The index=False parameter ensures that the DataFrame index is not included in the CSV file.\n",
    "\n",
    "# How to Use\n",
    "1. Ensure you have the required libraries (BeautifulSoup, requests, pandas) installed.\n",
    "\n",
    "2. Run the script. It fetches the HTML content, parses it, and extracts comments from the specified webpage.\n",
    "\n",
    "3. The script generates a CSV file named 'cancer_research.csv' in the same directory where the script is located. This file contains the cleaned comments along with their corresponding 'type' ('cancer')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL of the forum site\n",
    "url_link =  'https://www.cancerresearchuk.org/about-cancer/cancer-chat/thread/chemo-side-effects-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching the html of the site\n",
    "result = requests.get(url_link).text\n",
    "doc = BeautifulSoup(result, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching only the comments section within the forum\n",
    "heading = doc.find(class_ = \"field-item even\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking only the contents of the comments section \n",
    "p = heading.find_all('p',)\n",
    "comments = []\n",
    "for x in p:\n",
    "    comments.append(str(x.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering to remove any special characters and if the comments were less than 20 characters as it would not hold any significant value\n",
    "comments = ([s.replace('\\xa0',' ') for s in comments])\n",
    "comments = [x for x in comments if  len(x) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe of the comments\n",
    "df = pd.DataFrame({'comments':comments , 'type':'cancer'})\n",
    "#Creating an Output of the datafram onto current working directory\n",
    "df.to_csv('cancer_research.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12431723b33b5113ab49df85cdecc197ea0fe48c5381d99bd0aa39e3b81856ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
