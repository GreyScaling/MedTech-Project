{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\andyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a5bc622630a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#To get subjectivity and polarity, import textblob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# importing Regular Expressions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "# For dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Importing all nltk (natural language Toolkit) related libraries\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('all')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# importing Regular Expressions\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Bag of Words representation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('all_gender_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>First Submitted Date</th>\n",
       "      <th>Brief Summary</th>\n",
       "      <th>Detailed Description</th>\n",
       "      <th>Condition</th>\n",
       "      <th>Intervention</th>\n",
       "      <th>Eligibility Criteria</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>13-Mar-23</td>\n",
       "      <td>Health workers, especially those in patient-fa...</td>\n",
       "      <td>On 11 March 2020, the rapidly spreading novel ...</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>Biological: BNT162b2 COVID-19 Vaccine\\nBNT162b...</td>\n",
       "      <td>Inclusion Criteria:\\nHealthcare workers (aged ...</td>\n",
       "      <td>Sexes Eligible for Study: All</td>\n",
       "      <td>18 Years and older   (Adult, Older Adult)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>14-Mar-23</td>\n",
       "      <td>Our study has the objective of identifying the...</td>\n",
       "      <td>Introduction: COVID 19 pandemic continues to p...</td>\n",
       "      <td>COVID-19 Pandemic\\nObesity</td>\n",
       "      <td>Device: Obesity\\nthe objective of identifying ...</td>\n",
       "      <td>Inclusion Criteria:\\nAll patients presenting a...</td>\n",
       "      <td>Sexes Eligible for Study: All</td>\n",
       "      <td>18 Years to 80 Years   (Adult, Older Adult)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65</td>\n",
       "      <td>12-Mar-23</td>\n",
       "      <td>The aim of the project is to identify the prev...</td>\n",
       "      <td>In the follow-up of patients with Covid-19 inf...</td>\n",
       "      <td>Cardiac Complication\\nCOVID-19 Pneumonia</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>Inclusion Criteria:\\nPatient with previous hos...</td>\n",
       "      <td>Sexes Eligible for Study: All</td>\n",
       "      <td>18 Years to 90 Years   (Adult, Older Adult)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87</td>\n",
       "      <td>9-Mar-23</td>\n",
       "      <td>This descriptive study examines neutralizing a...</td>\n",
       "      <td>This research is a cross-sectional study with ...</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>Diagnostic Test: COVID-19 Antibody\\nThis descr...</td>\n",
       "      <td>Inclusion Criteria:\\nHealth workers who receiv...</td>\n",
       "      <td>Sexes Eligible for Study: All</td>\n",
       "      <td>18 Years to 60 Years   (Adult)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88</td>\n",
       "      <td>10-Mar-23</td>\n",
       "      <td>Healthcare workers are at the forefront agains...</td>\n",
       "      <td>This cross-sectional study will consist of a s...</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>Inclusion Criteria:\\nemployees of the Fondazio...</td>\n",
       "      <td>Sexes Eligible for Study: All</td>\n",
       "      <td>Child, Adult, Older Adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 First Submitted Date  \\\n",
       "0          30            13-Mar-23   \n",
       "1          45            14-Mar-23   \n",
       "2          65            12-Mar-23   \n",
       "3          87             9-Mar-23   \n",
       "4          88            10-Mar-23   \n",
       "\n",
       "                                       Brief Summary  \\\n",
       "0  Health workers, especially those in patient-fa...   \n",
       "1  Our study has the objective of identifying the...   \n",
       "2  The aim of the project is to identify the prev...   \n",
       "3  This descriptive study examines neutralizing a...   \n",
       "4  Healthcare workers are at the forefront agains...   \n",
       "\n",
       "                                Detailed Description  \\\n",
       "0  On 11 March 2020, the rapidly spreading novel ...   \n",
       "1  Introduction: COVID 19 pandemic continues to p...   \n",
       "2  In the follow-up of patients with Covid-19 inf...   \n",
       "3  This research is a cross-sectional study with ...   \n",
       "4  This cross-sectional study will consist of a s...   \n",
       "\n",
       "                                  Condition  \\\n",
       "0                                  COVID-19   \n",
       "1                COVID-19 Pandemic\\nObesity   \n",
       "2  Cardiac Complication\\nCOVID-19 Pneumonia   \n",
       "3                                  COVID-19   \n",
       "4                                  COVID-19   \n",
       "\n",
       "                                        Intervention  \\\n",
       "0  Biological: BNT162b2 COVID-19 Vaccine\\nBNT162b...   \n",
       "1  Device: Obesity\\nthe objective of identifying ...   \n",
       "2                                       Not Provided   \n",
       "3  Diagnostic Test: COVID-19 Antibody\\nThis descr...   \n",
       "4                                       Not Provided   \n",
       "\n",
       "                                Eligibility Criteria  \\\n",
       "0  Inclusion Criteria:\\nHealthcare workers (aged ...   \n",
       "1  Inclusion Criteria:\\nAll patients presenting a...   \n",
       "2  Inclusion Criteria:\\nPatient with previous hos...   \n",
       "3  Inclusion Criteria:\\nHealth workers who receiv...   \n",
       "4  Inclusion Criteria:\\nemployees of the Fondazio...   \n",
       "\n",
       "                          Gender                                         Ages  \n",
       "0  Sexes Eligible for Study: All    18 Years and older   (Adult, Older Adult)  \n",
       "1  Sexes Eligible for Study: All  18 Years to 80 Years   (Adult, Older Adult)  \n",
       "2  Sexes Eligible for Study: All  18 Years to 90 Years   (Adult, Older Adult)  \n",
       "3  Sexes Eligible for Study: All               18 Years to 60 Years   (Adult)  \n",
       "4  Sexes Eligible for Study: All                    Child, Adult, Older Adult  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting lemmatizer variable\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopword = set(stopwords.words('english'))\n",
    "stopword.update({'the','will','and','bnt162b2','to','of'}) # to remove other common words, but it didn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Tweet data\n",
    "def website_processor(website_data):\n",
    "    \n",
    "    #Changing all tweet texts to be lowercase\n",
    "    website_data = str(website_data).lower()\n",
    "    \n",
    "    #Removing punctuation\n",
    "    website_data = re.sub('\\[.*?\\]', '', website_data) #re.sub(what we want to replace, what we replace with, the dataset)\n",
    "\n",
    "    #Removing random standalone alphabets \n",
    "    website_data = re.sub(\"[^a-z\\s]\", \"\", website_data)\n",
    "\n",
    "    #Removing hashtags\n",
    "    website_data = re.sub(\"#\", \" \", website_data)\n",
    "\n",
    "    #Removing RT \n",
    "    website_data = re.sub(\"RT[\\s]+\", \"\", website_data)\n",
    "\n",
    "    #Removing hyperlinks\n",
    "    website_data = re.sub('https?://\\S+|www\\.\\S+', '', website_data)\n",
    "\n",
    "    website_data = re.sub('<.*?>+', '', website_data)\n",
    "\n",
    "    website_data = re.sub('[%s]' % re.escape(string.punctuation), '', website_data)\n",
    "\n",
    "    #Removing lines separated by \\n\n",
    "    website_data = re.sub('\\n', '', website_data)\n",
    "\n",
    "    website_data = re.sub('\\w*\\d\\w*', '', website_data) #second code satisfy those conditions that weren't covered in the first line of code (Above)\n",
    "    \n",
    "    #Removing all the numbers\n",
    "    website_data = re.sub(r'[0-9]',' ', website_data) #try to remove the number but it didn't remove\n",
    "    \n",
    "    #Removing stopwords\n",
    "    website_data = [word for word in website_data.split(' ') if word not in stopword]\n",
    "    website_data =\" \".join(website_data)\n",
    "    #.split(' ') splits the sentence by the empty spaces\n",
    "    #' '.join joins all the data in tweet data with a single spacing\n",
    "    \n",
    "    #Lemmatize \n",
    "    website_data = [lemmatizer.lemmatize(word) for word in website_data.split(' ')] #making all the words into their base form\n",
    "    website_data=\" \".join(website_data)\n",
    "    #website_data=\",\".join([str(i) for i in website_data])\n",
    "    return website_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable for our processed data\n",
    "processed_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       health worker especially patientfacing role si...\n",
       "1       study objective identifying epidemiological pr...\n",
       "2       aim project identify prevalence characteristic...\n",
       "3       descriptive study examines neutralizing antibo...\n",
       "4       healthcare worker forefront covid worldwide si...\n",
       "                              ...                        \n",
       "4251    neurorx developing nrx fixeddose combination o...\n",
       "4252    purpose study determine whether psilocybinassi...\n",
       "4253    purpose study evaluate effect mobile intervent...\n",
       "4254    projectevo videogame based intervention target...\n",
       "4255    large scale effective lowcost evidencebased pr...\n",
       "Name: Brief Summary, Length: 4256, dtype: object"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data['Brief Summary'].apply(website_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = CountVectorizer(ngram_range = (1,1)).fit(processed_data['Brief Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0003',\n",
       " '001',\n",
       " '002',\n",
       " '002a',\n",
       " '006',\n",
       " '009',\n",
       " '01',\n",
       " '011',\n",
       " '0149',\n",
       " '02',\n",
       " '029',\n",
       " '03',\n",
       " '04',\n",
       " '042',\n",
       " '0451',\n",
       " '04995274',\n",
       " '05',\n",
       " '051',\n",
       " '06',\n",
       " '061',\n",
       " '06946860',\n",
       " '07',\n",
       " '07321332',\n",
       " '075',\n",
       " '078',\n",
       " '08',\n",
       " '080',\n",
       " '081',\n",
       " '083',\n",
       " '09',\n",
       " '0903',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '100000',\n",
       " '1000a',\n",
       " '1008',\n",
       " '100mg',\n",
       " '100â',\n",
       " '101',\n",
       " '1018',\n",
       " '102',\n",
       " '104',\n",
       " '105',\n",
       " '1050',\n",
       " '1056',\n",
       " '106',\n",
       " '107',\n",
       " '1076',\n",
       " '108',\n",
       " '1083',\n",
       " '1093',\n",
       " '1099',\n",
       " '10b',\n",
       " '10mewt',\n",
       " '10mg',\n",
       " '10ml',\n",
       " '10th',\n",
       " '10ui',\n",
       " '10â',\n",
       " '11',\n",
       " '1100',\n",
       " '11006',\n",
       " '1105',\n",
       " '111',\n",
       " '1118',\n",
       " '112',\n",
       " '114',\n",
       " '1152',\n",
       " '117',\n",
       " '118',\n",
       " '119',\n",
       " '11c',\n",
       " '11c0112',\n",
       " '11months',\n",
       " '11th',\n",
       " '11year',\n",
       " '11â',\n",
       " '12',\n",
       " '120',\n",
       " '1200',\n",
       " '1202',\n",
       " '1203',\n",
       " '1210',\n",
       " '122',\n",
       " '1222',\n",
       " '123',\n",
       " '124',\n",
       " '125',\n",
       " '1250',\n",
       " '126',\n",
       " '127',\n",
       " '1273',\n",
       " '127â',\n",
       " '128',\n",
       " '129',\n",
       " '1299',\n",
       " '12h',\n",
       " '12hz',\n",
       " '12th',\n",
       " '13',\n",
       " '130',\n",
       " '1309',\n",
       " '131',\n",
       " '1316',\n",
       " '131i',\n",
       " '132',\n",
       " '1320',\n",
       " '1331',\n",
       " '135',\n",
       " '136',\n",
       " '137',\n",
       " '138',\n",
       " '13th',\n",
       " '14',\n",
       " '140',\n",
       " '142',\n",
       " '144',\n",
       " '145',\n",
       " '147',\n",
       " '148',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '15000',\n",
       " '1501',\n",
       " '150ml',\n",
       " '151',\n",
       " '1518',\n",
       " '152',\n",
       " '1524',\n",
       " '153',\n",
       " '1537',\n",
       " '154',\n",
       " '155',\n",
       " '1556023',\n",
       " '1558',\n",
       " '155mg',\n",
       " '156',\n",
       " '15ml',\n",
       " '15mm',\n",
       " '15th',\n",
       " '16',\n",
       " '160',\n",
       " '1600',\n",
       " '160ppm',\n",
       " '162b2',\n",
       " '163',\n",
       " '164',\n",
       " '1650',\n",
       " '166',\n",
       " '169',\n",
       " '16th',\n",
       " '17',\n",
       " '1701963',\n",
       " '171',\n",
       " '172',\n",
       " '173',\n",
       " '175',\n",
       " '176',\n",
       " '177',\n",
       " '177lu',\n",
       " '178',\n",
       " '179',\n",
       " '17938',\n",
       " '17a',\n",
       " '17h',\n",
       " '17m',\n",
       " '17th',\n",
       " '18',\n",
       " '180',\n",
       " '180mg',\n",
       " '181',\n",
       " '185',\n",
       " '188',\n",
       " '18f',\n",
       " '18h',\n",
       " '18kda',\n",
       " '18m',\n",
       " '18min',\n",
       " '18months',\n",
       " '18th',\n",
       " '19',\n",
       " '190',\n",
       " '1901',\n",
       " '1902',\n",
       " '193',\n",
       " '1930s',\n",
       " '194',\n",
       " '1945',\n",
       " '195',\n",
       " '196',\n",
       " '1965',\n",
       " '1970s',\n",
       " '198',\n",
       " '1990s',\n",
       " '1993',\n",
       " '1994',\n",
       " '1999',\n",
       " '19by',\n",
       " '19f',\n",
       " '19g',\n",
       " '19s',\n",
       " '19vaccine',\n",
       " '19will',\n",
       " '19â',\n",
       " '19ï¼',\n",
       " '1a',\n",
       " '1am',\n",
       " '1and',\n",
       " '1b',\n",
       " '1e6cells',\n",
       " '1expression',\n",
       " '1g',\n",
       " '1gbq',\n",
       " '1gr',\n",
       " '1l',\n",
       " '1mg',\n",
       " '1million',\n",
       " '1ml',\n",
       " '1objective',\n",
       " '1r',\n",
       " '1ra',\n",
       " '1s',\n",
       " '1st',\n",
       " '1th',\n",
       " '1v',\n",
       " '1x1',\n",
       " '1î',\n",
       " '1î²',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '200000',\n",
       " '2000s',\n",
       " '2001',\n",
       " '2002',\n",
       " '2003',\n",
       " '2005',\n",
       " '2006',\n",
       " '2007',\n",
       " '2008',\n",
       " '2009',\n",
       " '200mg',\n",
       " '201',\n",
       " '2010',\n",
       " '2010201136',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2016',\n",
       " '2017',\n",
       " '2018',\n",
       " '2018279015',\n",
       " '2019',\n",
       " '2019ncov',\n",
       " '2019ï¼ˆcovid',\n",
       " '202',\n",
       " '2020',\n",
       " '20203',\n",
       " '2021',\n",
       " '2021b',\n",
       " '2022',\n",
       " '2023',\n",
       " '2030',\n",
       " '204',\n",
       " '206',\n",
       " '209',\n",
       " '20g',\n",
       " '20h',\n",
       " '20i',\n",
       " '20j',\n",
       " '20mg',\n",
       " '20ml',\n",
       " '20vpnc',\n",
       " '20î¼g',\n",
       " '21',\n",
       " '210',\n",
       " '2109',\n",
       " '211',\n",
       " '214',\n",
       " '215',\n",
       " '216',\n",
       " '219',\n",
       " '21st',\n",
       " '22',\n",
       " '220',\n",
       " '221307',\n",
       " '222',\n",
       " '223',\n",
       " '228',\n",
       " '2298',\n",
       " '22g',\n",
       " '22mm',\n",
       " '22â',\n",
       " '23',\n",
       " '230',\n",
       " '232',\n",
       " '233',\n",
       " '234',\n",
       " '236',\n",
       " '24',\n",
       " '240',\n",
       " '2400',\n",
       " '246',\n",
       " '24h',\n",
       " '24th',\n",
       " '24â',\n",
       " '25',\n",
       " '250',\n",
       " '2500',\n",
       " '2508',\n",
       " '250mg',\n",
       " '251',\n",
       " '2539567',\n",
       " '256',\n",
       " '259',\n",
       " '25mg',\n",
       " '25oh',\n",
       " '25th',\n",
       " '26',\n",
       " '260',\n",
       " '265',\n",
       " '2660',\n",
       " '268',\n",
       " '26â',\n",
       " '27',\n",
       " '270',\n",
       " '271036826',\n",
       " '2765119',\n",
       " '278',\n",
       " '27th',\n",
       " '28',\n",
       " '2801',\n",
       " '282',\n",
       " '286',\n",
       " '28th',\n",
       " '29',\n",
       " '290',\n",
       " '2902a',\n",
       " '2905a',\n",
       " '291',\n",
       " '294',\n",
       " '2963131',\n",
       " '297',\n",
       " '298',\n",
       " '299v',\n",
       " '2a',\n",
       " '2and',\n",
       " '2b',\n",
       " '2b04',\n",
       " '2c',\n",
       " '2cm',\n",
       " '2d',\n",
       " '2d3',\n",
       " '2f',\n",
       " '2hr',\n",
       " '2l',\n",
       " '2mwt',\n",
       " '2nd',\n",
       " '2x2',\n",
       " '2x2x2',\n",
       " '2x3',\n",
       " '2ã',\n",
       " '30',\n",
       " '300',\n",
       " '300mg',\n",
       " '300mmhg',\n",
       " '301',\n",
       " '302',\n",
       " '305',\n",
       " '3051',\n",
       " '306',\n",
       " '30mg',\n",
       " '30ml',\n",
       " '30th',\n",
       " '30âµg',\n",
       " '31',\n",
       " '310',\n",
       " '3109',\n",
       " '311',\n",
       " '312',\n",
       " '31219',\n",
       " '314',\n",
       " '3162ï¼',\n",
       " '319',\n",
       " '31st',\n",
       " '32',\n",
       " '320',\n",
       " '324',\n",
       " '325',\n",
       " '33',\n",
       " '336',\n",
       " '34',\n",
       " '340',\n",
       " '342',\n",
       " '343',\n",
       " '344',\n",
       " '3475',\n",
       " '349',\n",
       " '35',\n",
       " '350',\n",
       " '3500',\n",
       " '3502',\n",
       " '350a',\n",
       " '351',\n",
       " '35500',\n",
       " '35mg',\n",
       " '36',\n",
       " '360',\n",
       " '361',\n",
       " '363',\n",
       " '366',\n",
       " '368',\n",
       " '37',\n",
       " '375',\n",
       " '375mg',\n",
       " '3769',\n",
       " '38',\n",
       " '380',\n",
       " '384',\n",
       " '385',\n",
       " '3858',\n",
       " '387',\n",
       " '39',\n",
       " '392',\n",
       " '394',\n",
       " '3a',\n",
       " '3b',\n",
       " '3cm',\n",
       " '3d',\n",
       " '3mg',\n",
       " '3ml',\n",
       " '3mm',\n",
       " '3months',\n",
       " '3p',\n",
       " '3prgd2',\n",
       " '3prgd2ï¼',\n",
       " '3rd',\n",
       " '3t',\n",
       " '3times',\n",
       " '3weeks',\n",
       " '40',\n",
       " '400',\n",
       " '4000',\n",
       " '400mg',\n",
       " '400î¼l',\n",
       " '404',\n",
       " '407',\n",
       " '40î¼g',\n",
       " '41',\n",
       " '414',\n",
       " '415',\n",
       " '42',\n",
       " '420',\n",
       " '42152',\n",
       " '427',\n",
       " '43',\n",
       " '430',\n",
       " '44',\n",
       " '4419',\n",
       " '443',\n",
       " '4482',\n",
       " '449',\n",
       " '44â',\n",
       " '45',\n",
       " '450',\n",
       " '452',\n",
       " '45mg',\n",
       " '46',\n",
       " '460',\n",
       " '4600',\n",
       " '465',\n",
       " '47',\n",
       " '471',\n",
       " '479',\n",
       " '47d11',\n",
       " '48',\n",
       " '480',\n",
       " '4863',\n",
       " '488210',\n",
       " '488211',\n",
       " '48h',\n",
       " '49',\n",
       " '4c',\n",
       " '4d',\n",
       " '4dcbct',\n",
       " '4h',\n",
       " '4k',\n",
       " '4l6715',\n",
       " '4th',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '5003',\n",
       " '500mg',\n",
       " '501',\n",
       " '501y',\n",
       " '503',\n",
       " '50hz',\n",
       " '50mg',\n",
       " '50ð',\n",
       " '51',\n",
       " '5131a',\n",
       " '5138',\n",
       " '514',\n",
       " '517',\n",
       " '52',\n",
       " '527',\n",
       " '53',\n",
       " '531',\n",
       " '539',\n",
       " '53â',\n",
       " '53î¼g',\n",
       " '54',\n",
       " '540',\n",
       " '547',\n",
       " '549',\n",
       " '55',\n",
       " '550',\n",
       " '56',\n",
       " '560',\n",
       " '5601745',\n",
       " '562',\n",
       " '5656',\n",
       " '5670a',\n",
       " '57',\n",
       " '5734',\n",
       " '575',\n",
       " '58',\n",
       " '5873',\n",
       " '588210',\n",
       " '59',\n",
       " '591',\n",
       " '596',\n",
       " '5cm',\n",
       " '5d',\n",
       " '5days',\n",
       " '5g',\n",
       " '5ht4',\n",
       " '5l',\n",
       " '5mg',\n",
       " '5ml',\n",
       " '5pm',\n",
       " '5th',\n",
       " '5ug',\n",
       " '5x1010',\n",
       " '60',\n",
       " '600',\n",
       " '603',\n",
       " '60kg',\n",
       " '60mg',\n",
       " '61',\n",
       " '612',\n",
       " '613',\n",
       " '62',\n",
       " '624',\n",
       " '629',\n",
       " '63',\n",
       " '64',\n",
       " '640',\n",
       " '645',\n",
       " '65',\n",
       " '650',\n",
       " '651',\n",
       " '658',\n",
       " '65y',\n",
       " '66',\n",
       " '660',\n",
       " '666',\n",
       " '67',\n",
       " '670',\n",
       " '675',\n",
       " '68',\n",
       " '68154',\n",
       " '684',\n",
       " '685',\n",
       " '686',\n",
       " '68ga',\n",
       " '69',\n",
       " '690',\n",
       " '69gy',\n",
       " '6mg',\n",
       " '6months',\n",
       " '6mwt',\n",
       " '6r',\n",
       " '6th',\n",
       " '6ug',\n",
       " '6x',\n",
       " '6â',\n",
       " '70',\n",
       " '700',\n",
       " '704',\n",
       " '706',\n",
       " '707',\n",
       " '71',\n",
       " '7123',\n",
       " '715600',\n",
       " '718',\n",
       " '72',\n",
       " '72h',\n",
       " '73',\n",
       " '731',\n",
       " '739',\n",
       " '73m',\n",
       " '73m2',\n",
       " '74',\n",
       " '741',\n",
       " '74699157',\n",
       " '75',\n",
       " '750',\n",
       " '755',\n",
       " '76',\n",
       " '760',\n",
       " '77',\n",
       " '770',\n",
       " '773',\n",
       " '78',\n",
       " '780',\n",
       " '7831',\n",
       " '79',\n",
       " '790',\n",
       " '7cells',\n",
       " '7cups',\n",
       " '7gbq',\n",
       " '7th',\n",
       " '80',\n",
       " '800',\n",
       " '8000',\n",
       " '800mg',\n",
       " '806',\n",
       " '80â',\n",
       " '81',\n",
       " '810nm',\n",
       " '812',\n",
       " '813',\n",
       " '814',\n",
       " '81694',\n",
       " '82',\n",
       " '820',\n",
       " '822',\n",
       " '824',\n",
       " '83',\n",
       " '835',\n",
       " '838',\n",
       " '84',\n",
       " '8430',\n",
       " '85',\n",
       " '85mg',\n",
       " '86',\n",
       " '87',\n",
       " '872',\n",
       " '875',\n",
       " '8752',\n",
       " '876',\n",
       " '88',\n",
       " '89',\n",
       " '895',\n",
       " '89zr',\n",
       " '8cells',\n",
       " '8k',\n",
       " '8mg',\n",
       " '8months',\n",
       " '8th',\n",
       " '90',\n",
       " '900',\n",
       " '91',\n",
       " '919',\n",
       " '92',\n",
       " '9229',\n",
       " '93',\n",
       " '94',\n",
       " '942',\n",
       " '95',\n",
       " '95005',\n",
       " '96',\n",
       " '960',\n",
       " '97',\n",
       " '979',\n",
       " '98',\n",
       " '981',\n",
       " '986205',\n",
       " '986310',\n",
       " '99',\n",
       " '99mtc',\n",
       " '9mm',\n",
       " '9th',\n",
       " '9â',\n",
       " 'a1',\n",
       " 'a1c',\n",
       " 'a2',\n",
       " 'a3',\n",
       " 'a317',\n",
       " 'aai',\n",
       " 'aand',\n",
       " 'aap',\n",
       " 'aarhus',\n",
       " 'aat',\n",
       " 'aaz',\n",
       " 'ab',\n",
       " 'ab122',\n",
       " 'ab201',\n",
       " 'ab928',\n",
       " 'abandoned',\n",
       " 'abandonment',\n",
       " 'abbc1',\n",
       " 'abbott',\n",
       " 'abbreviated',\n",
       " 'abbv',\n",
       " 'abc',\n",
       " 'abd',\n",
       " 'abda',\n",
       " 'abdomen',\n",
       " 'abdominal',\n",
       " 'abdominis',\n",
       " 'abdominoperineal',\n",
       " 'abdominus',\n",
       " 'abemaciclib',\n",
       " 'aberrant',\n",
       " 'aberration',\n",
       " 'aberrations',\n",
       " 'abi',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abiraterone',\n",
       " 'abivertinib',\n",
       " 'abl1',\n",
       " 'ablation',\n",
       " 'ablations',\n",
       " 'able',\n",
       " 'abm',\n",
       " 'abncov2',\n",
       " 'abnormal',\n",
       " 'abnormalities',\n",
       " 'abnormality',\n",
       " 'abnornal',\n",
       " 'abo',\n",
       " 'abolishing',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abraxane',\n",
       " 'abraxaneâ',\n",
       " 'abroad',\n",
       " 'abscesses',\n",
       " 'absence',\n",
       " 'absent',\n",
       " 'absenteeism',\n",
       " 'absolute',\n",
       " 'absorb',\n",
       " 'absorbed',\n",
       " 'absorption',\n",
       " 'abstract',\n",
       " 'abstracted',\n",
       " 'abstraction',\n",
       " 'abu',\n",
       " 'abuja',\n",
       " 'abulia',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abuse',\n",
       " 'abusers',\n",
       " 'aby',\n",
       " 'ac',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academies',\n",
       " 'academy',\n",
       " 'acalabrutinib',\n",
       " 'acbt',\n",
       " 'acc',\n",
       " 'accelerate',\n",
       " 'accelerated',\n",
       " 'accelerates',\n",
       " 'accelerating',\n",
       " 'accelerometer',\n",
       " 'accentuated',\n",
       " 'accept',\n",
       " 'acceptability',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'access',\n",
       " 'accessed',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accessing',\n",
       " 'accessory',\n",
       " 'accident',\n",
       " 'accidents',\n",
       " 'accommodation',\n",
       " 'accommodations',\n",
       " 'accompanied',\n",
       " 'accompany',\n",
       " 'accompanying',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishing',\n",
       " 'accomplishments',\n",
       " 'accord',\n",
       " 'accordance',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'accounted',\n",
       " 'accounting',\n",
       " 'accounts',\n",
       " 'accredited',\n",
       " 'accrual',\n",
       " 'accrued',\n",
       " 'accumulate',\n",
       " 'accumulated',\n",
       " 'accumulating',\n",
       " 'accumulation',\n",
       " 'accumulations',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'ace',\n",
       " 'ace1',\n",
       " 'ace2',\n",
       " 'ace2r',\n",
       " 'acei',\n",
       " 'aceis',\n",
       " 'acetaminophen',\n",
       " 'acetate',\n",
       " 'acetic',\n",
       " 'acetoacetate',\n",
       " 'acetyl',\n",
       " 'acetylcholine',\n",
       " 'acetylcystein',\n",
       " 'acetylcysteine',\n",
       " 'acetylglucosamine',\n",
       " 'aches',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'achieving',\n",
       " 'acid',\n",
       " 'acidic',\n",
       " 'acids',\n",
       " 'acinar',\n",
       " 'acknowledge',\n",
       " 'acknowledged',\n",
       " 'acknowledgment',\n",
       " 'acl',\n",
       " 'acobiom',\n",
       " 'acoi',\n",
       " 'acp',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquiring',\n",
       " 'acquisition',\n",
       " 'acquisitions',\n",
       " 'acr',\n",
       " 'acral',\n",
       " 'acronym',\n",
       " 'across',\n",
       " 'acrylic',\n",
       " 'acs',\n",
       " 'acsm',\n",
       " 'act',\n",
       " 'acth',\n",
       " 'acti',\n",
       " 'actigraph',\n",
       " 'actigraphy',\n",
       " 'actin',\n",
       " 'acting',\n",
       " 'actinic',\n",
       " 'action',\n",
       " 'actionable',\n",
       " 'actions',\n",
       " 'activ',\n",
       " 'activatable',\n",
       " 'activate',\n",
       " 'activated',\n",
       " 'activates',\n",
       " 'activating',\n",
       " 'activation',\n",
       " 'activator',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'actives',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'activitylink',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'acts',\n",
       " 'actt',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'actuated',\n",
       " 'acu',\n",
       " 'acuity',\n",
       " 'acupressure',\n",
       " 'acupuncture',\n",
       " 'acupuncturist',\n",
       " 'acut',\n",
       " 'acute',\n",
       " 'acutely',\n",
       " 'acylethanolamine',\n",
       " 'acã',\n",
       " 'ad',\n",
       " 'ad26',\n",
       " 'ad5',\n",
       " 'adagrasib',\n",
       " 'adapt',\n",
       " 'adaptation',\n",
       " 'adaptations',\n",
       " 'adaptative',\n",
       " 'adapted',\n",
       " 'adapting',\n",
       " 'adaptive',\n",
       " 'adapts',\n",
       " 'adas',\n",
       " 'adc',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addicted',\n",
       " 'addiction',\n",
       " 'addictive',\n",
       " 'adding',\n",
       " 'additiion',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'additive',\n",
       " 'additively',\n",
       " 'addmission',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addresses',\n",
       " 'addressing',\n",
       " 'adds',\n",
       " 'ade',\n",
       " 'adechotech',\n",
       " 'adenocarcinoma',\n",
       " 'adenocarcinomas',\n",
       " 'adenoid',\n",
       " 'adenoma',\n",
       " 'adenomas',\n",
       " 'adenomatous',\n",
       " 'adenovirus',\n",
       " 'adequacy',\n",
       " 'adequate',\n",
       " 'adequately',\n",
       " 'adhd',\n",
       " 'adhera',\n",
       " 'adhere',\n",
       " 'adherence',\n",
       " 'adherenet',\n",
       " 'adherent',\n",
       " 'adhering',\n",
       " 'adhesion',\n",
       " 'adhesive',\n",
       " 'adhoc',\n",
       " 'adimrsc',\n",
       " 'adipocytes',\n",
       " 'adipose',\n",
       " 'adiposity',\n",
       " 'adjacent',\n",
       " 'adjuavnt',\n",
       " 'adjudicated',\n",
       " 'adjunct',\n",
       " 'adjunctive',\n",
       " 'adjust',\n",
       " 'adjustable',\n",
       " 'adjusted',\n",
       " 'adjusting',\n",
       " 'adjustment',\n",
       " 'adjustments',\n",
       " 'adjusts',\n",
       " 'adjuvant',\n",
       " 'adjuvanted',\n",
       " 'adjuvants',\n",
       " 'adl',\n",
       " 'administer',\n",
       " 'administered',\n",
       " 'administering',\n",
       " 'administers',\n",
       " 'administrable',\n",
       " 'administrate',\n",
       " 'administrated',\n",
       " 'administrates',\n",
       " 'administrating',\n",
       " 'administration',\n",
       " 'administrations',\n",
       " 'administrative',\n",
       " 'administrators',\n",
       " 'admiration',\n",
       " 'admission',\n",
       " 'admissions',\n",
       " ...]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = matrix.get_feature_names()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = processed_data['Brief Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = matrix.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 185)\t1\n",
      "  (0, 1143)\t1\n",
      "  (0, 1163)\t1\n",
      "  (0, 1251)\t1\n",
      "  (0, 1342)\t2\n",
      "  (0, 1410)\t4\n",
      "  (0, 1823)\t1\n",
      "  (0, 1874)\t1\n",
      "  (0, 2204)\t2\n",
      "  (0, 2499)\t2\n",
      "  (0, 2815)\t1\n",
      "  (0, 2912)\t1\n",
      "  (0, 3513)\t1\n",
      "  (0, 3550)\t1\n",
      "  (0, 3792)\t1\n",
      "  (0, 3978)\t1\n",
      "  (0, 4197)\t1\n",
      "  (0, 4499)\t1\n",
      "  (0, 5419)\t1\n",
      "  (0, 5509)\t2\n",
      "  (0, 5744)\t1\n",
      "  (0, 6001)\t1\n",
      "  (0, 6356)\t1\n",
      "  (0, 6683)\t1\n",
      "  (0, 6752)\t2\n",
      "  :\t:\n",
      "  (4255, 14283)\t1\n",
      "  (4255, 14430)\t1\n",
      "  (4255, 14718)\t1\n",
      "  (4255, 14877)\t1\n",
      "  (4255, 15339)\t2\n",
      "  (4255, 15424)\t1\n",
      "  (4255, 15502)\t3\n",
      "  (4255, 15506)\t1\n",
      "  (4255, 15840)\t1\n",
      "  (4255, 15882)\t1\n",
      "  (4255, 15923)\t3\n",
      "  (4255, 15926)\t2\n",
      "  (4255, 15930)\t1\n",
      "  (4255, 15951)\t1\n",
      "  (4255, 15997)\t5\n",
      "  (4255, 16142)\t6\n",
      "  (4255, 16251)\t1\n",
      "  (4255, 16317)\t1\n",
      "  (4255, 16776)\t1\n",
      "  (4255, 16802)\t1\n",
      "  (4255, 17026)\t1\n",
      "  (4255, 17295)\t1\n",
      "  (4255, 17304)\t1\n",
      "  (4255, 17337)\t1\n",
      "  (4255, 17342)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)  # the coordinate (matrix entry (i,j)) represents (the nth row data, the phrase/words), the number represents how many times the phrase/words appeared in the nth row data\n",
    "# the rest is implied to the 0. So X is a 4255 by 17342 matrix (number of rows of data, number of features name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0003</th>\n",
       "      <th>001</th>\n",
       "      <th>002</th>\n",
       "      <th>002a</th>\n",
       "      <th>006</th>\n",
       "      <th>009</th>\n",
       "      <th>01</th>\n",
       "      <th>011</th>\n",
       "      <th>...</th>\n",
       "      <th>âµl</th>\n",
       "      <th>ãÿ</th>\n",
       "      <th>ãžle</th>\n",
       "      <th>î²</th>\n",
       "      <th>î³</th>\n",
       "      <th>îºb</th>\n",
       "      <th>î¼g</th>\n",
       "      <th>î¼m</th>\n",
       "      <th>ï¼œand</th>\n",
       "      <th>ï¼ˆq3wï¼</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4253</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4254</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4255</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4256 rows × 17567 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  0003  001  002  002a  006  009  01  011  ...  âµl  ãÿ  ãžle  \\\n",
       "0      0    0     0    0    0     0    0    0   0    0  ...    0   0     0   \n",
       "1      0    0     0    0    0     0    0    0   0    0  ...    0   0     0   \n",
       "2      0    0     0    0    0     0    0    0   0    0  ...    0   0     0   \n",
       "3      0    0     0    0    0     0    0    0   0    0  ...    0   0     0   \n",
       "4      0    0     0    0    0     0    0    0   0    0  ...    0   0     0   \n",
       "...   ..  ...   ...  ...  ...   ...  ...  ...  ..  ...  ...  ...  ..   ...   \n",
       "4251   0    0     0    0    0     0    0    0   0    0  ...    0   0     0   \n",
       "4252   0    0     0    0    0     0    0    0   0    0  ...    0   0     0   \n",
       "4253   0    0     0    0    0     0    0    0   0    0  ...    0   0     0   \n",
       "4254   0    0     0    0    0     0    0    0   0    0  ...    0   0     0   \n",
       "4255   0    0     0    0    0     0    0    0   0    0  ...    0   0     0   \n",
       "\n",
       "      î²  î³  îºb  î¼g  î¼m  ï¼œand  ï¼ˆq3wï¼  \n",
       "0      0   0    0    0    0       0         0  \n",
       "1      0   0    0    0    0       0         0  \n",
       "2      0   0    0    0    0       0         0  \n",
       "3      0   0    0    0    0       0         0  \n",
       "4      0   0    0    0    0       0         0  \n",
       "...   ..  ..  ...  ...  ...     ...       ...  \n",
       "4251   0   0    0    0    0       0         0  \n",
       "4252   0   0    0    0    0       0         0  \n",
       "4253   0   0    0    0    0       0         0  \n",
       "4254   0   0    0    0    0       0         0  \n",
       "4255   0   0    0    0    0       0         0  \n",
       "\n",
       "[4256 rows x 17567 columns]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df = pd.DataFrame(X.toarray(), columns = feature_names)\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4246</th>\n",
       "      <th>4247</th>\n",
       "      <th>4248</th>\n",
       "      <th>4249</th>\n",
       "      <th>4250</th>\n",
       "      <th>4251</th>\n",
       "      <th>4252</th>\n",
       "      <th>4253</th>\n",
       "      <th>4254</th>\n",
       "      <th>4255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>will</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaccine</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workers</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 4256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "the        10     4     3     2     7     2    15     2     0    17  ...   \n",
       "of          7     3     2     1     3     2    11     2     1    16  ...   \n",
       "to          5     0     1     0     3     0    11     2     1     6  ...   \n",
       "health      4     0     0     1     0     0     0     0     0     0  ...   \n",
       "and         4     1     1     1     3     0     7     1     2    17  ...   \n",
       "will        3     0     0     0     1     0     1     0     0     9  ...   \n",
       "vaccine     3     0     0     1     0     0     0     0     0     0  ...   \n",
       "workers     3     0     0     1     2     0     0     0     0     0  ...   \n",
       "\n",
       "         4246  4247  4248  4249  4250  4251  4252  4253  4254  4255  \n",
       "the         2     5     4     3     4     3     1     4     5     2  \n",
       "of          2     1     2     2     3     4     1     4     7     3  \n",
       "to          1    12     1     2     2     4     1     3     7     6  \n",
       "health      0     0     0     1     0     0     0     2     0     2  \n",
       "and         4     2     0     1     2     6     1     1     4     8  \n",
       "will        1     1     0     0     2     3     0     2     1     0  \n",
       "vaccine     0     0     0     0     0     0     0     0     0     0  \n",
       "workers     0     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[8 rows x 4256 columns]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df.T.sort_values(by=0, ascending=False).head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword.update({'the','will','and','bnt162b2','to','of'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'bnt162b2',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
